---
title: OpenAI Integration
description: Trace and monitor OpenAI API calls with Brokle - including chat completions, embeddings, and function calling
---

import { Callout } from "fumadocs-ui/components/callout";
import { Tabs, Tab } from "fumadocs-ui/components/tabs";

# OpenAI Integration

Automatically trace all your OpenAI API calls with Brokle. Capture chat completions, embeddings, function calls, and more with minimal code changes.

```mermaid
sequenceDiagram
    participant App as Your App
    participant Wrapper as Brokle Wrapper
    participant OpenAI as OpenAI API
    participant Brokle as Brokle Platform

    App->>Wrapper: openai.chat.completions.create()
    Wrapper->>Wrapper: Start span, capture input
    Wrapper->>OpenAI: Forward request
    OpenAI-->>Wrapper: Response + usage
    Wrapper->>Wrapper: End span, capture tokens/cost
    Wrapper->>Brokle: Send trace data (async)
    Wrapper-->>App: Original response
```

## What Gets Traced

Brokle automatically captures:

| Feature | Data Captured |
|---------|--------------|
| **Chat Completions** | Messages, model, parameters, response, tokens, cost |
| **Embeddings** | Input text, model, dimensions, token count |
| **Function Calling** | Function definitions, arguments, results |
| **Tool Use** | Tool calls, responses, execution time |
| **Streaming** | Full streamed response, chunk timing |
| **Vision** | Image inputs (URL or base64 reference) |

## Installation

<Tabs>
  <Tab value="pip" label="pip">
    ```bash
    pip install brokle openai
    ```
  </Tab>
  <Tab value="poetry" label="Poetry">
    ```bash
    poetry add brokle openai
    ```
  </Tab>
  <Tab value="npm" label="npm">
    ```bash
    npm install brokle openai
    ```
  </Tab>
</Tabs>

## Quick Setup

The fastest way to get started is using the OpenAI wrapper:

<Tabs>
  <Tab value="python" label="Python">
    ```python
    from brokle import Brokle
    from brokle.wrappers import wrap_openai
    import openai

    # Initialize Brokle
    client = Brokle(api_key="your-brokle-api-key")

    # Wrap your OpenAI client
    openai_client = wrap_openai(openai.OpenAI())

    # Use as normal - tracing is automatic!
    response = openai_client.chat.completions.create(
        model="gpt-4",
        messages=[
            {"role": "user", "content": "What is the capital of France?"}
        ]
    )

    print(response.choices[0].message.content)
    # Output: The capital of France is Paris.
    ```
  </Tab>
  <Tab value="javascript" label="JavaScript">
    ```javascript
    import { Brokle } from 'brokle';
    import { wrapOpenAI } from 'brokle/openai';
    import OpenAI from 'openai';

    // Initialize Brokle
    const brokle = new Brokle({ apiKey: 'your-brokle-api-key' });

    // Wrap your OpenAI client
    const openai = wrapOpenAI(new OpenAI());

    // Use as normal - tracing is automatic!
    const response = await openai.chat.completions.create({
      model: 'gpt-4',
      messages: [
        { role: 'user', content: 'What is the capital of France?' }
      ]
    });

    console.log(response.choices[0].message.content);
    // Output: The capital of France is Paris.
    ```
  </Tab>
</Tabs>

<Callout type="success">
  That's it! All OpenAI calls will now appear in your Brokle dashboard with full request/response details, token counts, and costs.
</Callout>

## Configuration Options

Configuration is set globally via `Brokle()` or `get_client()` before wrapping your OpenAI client. The `wrap_openai()` function only accepts the client — it automatically uses the global Brokle configuration.

```python
from brokle import Brokle
from brokle.wrappers import wrap_openai
import openai

# Configure Brokle globally (before wrapping)
brokle = Brokle(
    api_key="bk_your_api_key",
    sample_rate=1.0,        # Sampling rate 0.0-1.0 (default: 1.0)
    tracing_enabled=True,   # Enable/disable tracing (default: True)
    debug=False,            # Enable debug logging (default: False)
    flush_at=100,           # Batch size before flush (default: 100)
    flush_interval=5.0,     # Max delay before flush in seconds (default: 5.0)
)

# Wrap OpenAI client (uses global Brokle config automatically)
openai_client = wrap_openai(openai.OpenAI())
```

| Option | Default | Description |
|--------|---------|-------------|
| `sample_rate` | `1.0` | Sampling rate for traces (0.0 to 1.0) |
| `tracing_enabled` | `True` | Enable/disable tracing entirely |
| `debug` | `False` | Enable debug logging |
| `mask` | `None` | Custom function to mask sensitive data before transmission |
| `flush_at` | `100` | Maximum batch size before flush (1-1000) |
| `flush_interval` | `5.0` | Maximum delay in seconds before flush (0.1-60.0) |
| `enabled` | `True` | Master switch to disable the SDK completely |

## Advanced Usage

### Streaming Responses

Streaming works seamlessly - Brokle captures the full response:

```python
openai_client = wrap_openai(openai.OpenAI())

stream = openai_client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "Tell me a story"}],
    stream=True
)

for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="")

# Brokle captures:
# - Time to first token
# - Complete streamed response
# - Token counts
# - Total duration
```

### Function Calling

Function calls are automatically traced with arguments and results:

```python
tools = [
    {
        "type": "function",
        "function": {
            "name": "get_weather",
            "description": "Get the current weather",
            "parameters": {
                "type": "object",
                "properties": {
                    "location": {"type": "string"}
                },
                "required": ["location"]
            }
        }
    }
]

response = openai_client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "What's the weather in Paris?"}],
    tools=tools
)

# Brokle traces:
# - Tool definitions
# - Function call arguments
# - Tool call ID
```

### Vision (GPT-4V)

Image inputs are traced with references:

```python
response = openai_client.chat.completions.create(
    model="gpt-4-vision-preview",
    messages=[
        {
            "role": "user",
            "content": [
                {"type": "text", "text": "What's in this image?"},
                {
                    "type": "image_url",
                    "image_url": {"url": "https://example.com/image.jpg"}
                }
            ]
        }
    ]
)

# Brokle traces the image URL (not the actual image data)
```

### Embeddings

Embedding calls are also traced:

```python
embeddings = openai_client.embeddings.create(
    model="text-embedding-3-small",
    input=["Hello world", "Goodbye world"]
)

# Brokle captures:
# - Input texts
# - Model
# - Dimensions
# - Token count
```

### Adding Context with Traces

Wrap OpenAI calls in traces for additional context:

```python
from brokle import Brokle
from brokle.wrappers import wrap_openai

client = Brokle(api_key="your-api-key")
openai_client = wrap_openai(openai.OpenAI())

# Add context to your traces
with client.start_as_current_span("customer_support_chat") as span:
    client.update_current_span(metadata={
        "user_id": "user_123",
        "conversation_id": "conv_456",
        "feature": "support_bot"
    })

    response = openai_client.chat.completions.create(
        model="gpt-4",
        messages=conversation_history
    )
```

## Cost Tracking

Brokle automatically calculates costs based on current OpenAI pricing:

| Model | Input (per 1M tokens) | Output (per 1M tokens) |
|-------|----------------------|------------------------|
| gpt-4-turbo | $10.00 | $30.00 |
| gpt-4 | $30.00 | $60.00 |
| gpt-4-vision | $10.00 | $30.00 |
| gpt-3.5-turbo | $0.50 | $1.50 |
| text-embedding-3-small | $0.02 | - |
| text-embedding-3-large | $0.13 | - |

<Callout type="info">
  Pricing is updated regularly. Check the Brokle dashboard for the most current rates or configure custom pricing for enterprise agreements.
</Callout>

## Troubleshooting

### Traces Not Appearing

1. **Check your API key** - Ensure `BROKLE_API_KEY` is set correctly
2. **Verify the wrapper** - Make sure you're using the wrapped client, not the original
3. **Check the project** - Ensure traces are going to the correct project

```python
# Debug: Print trace URL
with client.start_as_current_span("debug") as span:
    # Access trace context from the span
    response = openai_client.chat.completions.create(...)
```

### High Latency

Brokle tracing adds minimal overhead (under 5ms), but if you're experiencing issues:

1. **Use async mode** - Enable background uploading
2. **Batch uploads** - Configure batch size for high-throughput scenarios

```python
client = Brokle(
    api_key="your-api-key",
    async_mode=True,      # Upload in background
    batch_size=100        # Batch traces
)
```

### Missing Token Counts

Some OpenAI responses don't include usage data. Brokle estimates tokens when not provided.

## Related Integrations

- [Anthropic](/docs/integrations/anthropic) - Claude models
- [Azure OpenAI](/docs/integrations/azure-openai) - Azure-hosted OpenAI
- [LangChain](/docs/integrations/langchain) - LangChain framework

## Next Steps

- [View traces in the dashboard →](/docs/dashboard-tour)
- [Add evaluation scores →](/docs/evaluation)
- [Set up cost alerts →](/docs/analytics/cost-tracking)
